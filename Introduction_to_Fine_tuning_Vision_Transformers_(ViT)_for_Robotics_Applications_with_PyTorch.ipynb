{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jman4162/PyTorch-Vision-Transformers-ViT/blob/main/Introduction_to_Fine_tuning_Vision_Transformers_(ViT)_for_Robotics_Applications_with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECVjbPT2ffHE"
      },
      "source": [
        "# Introduction to Fine-tuning Vision Transformers (ViT) for Robotics Applications with PyTorch\n",
        "\n",
        "Name: John Hodge\n",
        "\n",
        "Data: 04/23/24"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In the fast-evolving field of robotics, visual perception is a critical component, enabling robots to navigate and interact with complex environments. One of the latest breakthroughs in computer vision is the adoption of Vision Transformers (ViTs), which have shown impressive results across a variety of tasks, rivaling traditional convolutional neural networks (CNNs). This tutorial aims to demonstrate the practical application of ViTs, specifically through fine-tuning the `vit_b_16` model for object recognition in robotics.\n",
        "\n",
        "### What is a Vision Transformer?\n",
        "\n",
        "Vision Transformers are a class of deep learning models adapted from transformers, which were originally developed for natural language processing. ViTs apply the transformer's self-attention mechanism to grids of image patches, allowing the model to weigh the importance of different parts of an image. This ability to focus on relevant image features adaptively is particularly useful in dynamic scenarios typical in robotics.\n",
        "\n",
        "The `vit_b_16` model, where \"b\" stands for \"base\" and \"16\" indicates the size of each image patch (16x16 pixels), is a medium-sized ViT model suitable for a wide range of vision tasks. It combines depth and complexity, offering a balanced trade-off between computational efficiency and accuracy.\n",
        "\n",
        "![ViT](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vit_architecture.jpg)\n",
        "\n",
        "Reference: [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)\n",
        "\n",
        "### Tutorial Overview\n",
        "\n",
        "This tutorial will guide you through the steps of fine-tuning the `vit_b_16` model using PyTorch, a powerful and flexible machine learning framework. We will cover:\n",
        "\n",
        "- **Setting up PyTorch and importing the ViT model**: How to load the pre-trained `vit_b_16` and prepare it for fine-tuning.\n",
        "- **Data preparation**: Techniques for preparing your image data for training and evaluation, tailored for robotic applications.\n",
        "- **Fine-tuning process**: Adjustments and optimization for the model specific to object recognition tasks in robotics.\n",
        "- **Evaluation and testing**: How to assess the model's performance to ensure it meets the demands of robotics applications.\n",
        "\n",
        "By the end of this tutorial, you will have a solid understanding of how to implement and adapt Vision Transformers for real-world robotic tasks, ensuring your robotic systems can benefit from the latest advancements in computer vision.\n",
        "\n",
        "Let's dive into the world of Vision Transformers and unlock powerful visual capabilities for your robotic applications!"
      ],
      "metadata": {
        "id": "tXxXbMNcGGsc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Environment\n",
        "First, ensure you have Python installed, and then install PyTorch and torchvision. You can install them using pip:"
      ],
      "metadata": {
        "id": "egaJobFTGskT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMq-2cEqe7ml",
        "outputId": "b69f40f6-2ee6-40f0-d3f9-ae38bade39ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchsummary tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Necessary Libraries"
      ],
      "metadata": {
        "id": "m2OGKmtrGz1F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95myTFzee8j-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torch import nn\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.optim as optim\n",
        "from torchvision.models import vit_b_16  # Import a pre-trained ViT model\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchsummary import summary as model_summary\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Torch: {torch.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlD9kTLd_18k",
        "outputId": "b18f64a6-13e8-482f-a34e-16550f559269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.2.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect to Google Drive to save models (optional)"
      ],
      "metadata": {
        "id": "77nLolnYG2WN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "CKQ18MSLtY2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5L2sWmXuFAc",
        "outputId": "6bab2f80-1655-4ab5-d648-4423b4deb632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set random seeds for repeatability"
      ],
      "metadata": {
        "id": "di2wUtrBJEeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "-z8EGn3D-tIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42  # You can choose any integer value\n",
        "seed_everything(seed)"
      ],
      "metadata": {
        "id": "PkUO-__FqPwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "\n",
        "For robotic applications, you might be interested in a dataset that includes images of objects that a robot might interact with. For demonstration, let's use a subset of CIFAR-10 dataset. Here's a detailed breakdown of each part:\n",
        "\n",
        "### 1. Define Transformations\n",
        "```python\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to fit ViT input dimensions\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "```\n",
        "- **Purpose**: This block of code defines the transformations that will be applied to each image in the dataset. These transformations are crucial for preparing the data for processing by the Vision Transformer model.\n",
        "- **Components**:\n",
        "  - `transforms.Resize((224, 224))`: Resizes each image to 224x224 pixels, which is a typical input size for Vision Transformers that were pretrained on ImageNet.\n",
        "  - `transforms.ToTensor()`: Converts the images to PyTorch tensors, which are a suitable format for model inputs.\n",
        "  - `transforms.Normalize(...)`: Normalizes the image data. This step adjusts the pixel values so that their distribution will have a mean and standard deviation that match the distribution of the dataset used to train the model initially (typically ImageNet).\n",
        "\n",
        "### 2. Load Datasets\n",
        "```python\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "```\n",
        "- **Purpose**: These lines load the CIFAR-10 dataset from disk, downloading it if it's not already available. The dataset is split into training data (`train=True`) and test data (`train=False`).\n",
        "- **Functionality**: The `transform` argument applies the previously defined transformations to the images as they are loaded, ensuring they are in the correct format and scale for training and testing.\n",
        "\n",
        "### 3. Split Training Dataset\n",
        "```python\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "```\n",
        "- **Purpose**: This part of the code splits the training dataset into two subsets: a training set and a validation set. The split is 80% for training and 20% for validation, which is a common ratio in machine learning to balance training and validation performance.\n",
        "- **Functionality**: `random_split` is used to randomly assign data points to each subset based on the sizes specified, ensuring that the validation set is representative of the overall dataset.\n",
        "\n",
        "### 4. Create DataLoaders\n",
        "```python\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "```\n",
        "- **Purpose**: These lines create DataLoader objects for the train, validation, and test datasets. DataLoaders are used to efficiently manage batches of data during the training and evaluation phases, providing necessary functionality like shuffling and parallel data loading.\n",
        "- **Functionality**:\n",
        "  - `shuffle=True` for the training DataLoader helps in randomizing the input data, which is beneficial for reducing model overfitting and improving model generalizability.\n",
        "  - `shuffle=False` for validation and test DataLoaders because the order of data does not impact performance evaluation and this helps in consistently evaluating the model.\n",
        "\n",
        "These steps comprehensively prepare the CIFAR-10 dataset for effective training and evaluation of a Vision Transformer model, ensuring data is in the proper format, normalized, and divided into appropriate sets for the training process."
      ],
      "metadata": {
        "id": "I5V7PFX8JB5K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQ7iauKZWXuj",
        "outputId": "bdfd61b6-abbb-4272-af06-3fb144ea990f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:04<00:00, 40836914.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Train Data: 40000\n",
            "Validation Data: 10000\n",
            "Test Data: 10000\n"
          ]
        }
      ],
      "source": [
        "# Define your transformations, assumed as 'transform' in your code\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to fit ViT input dimensions\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split train dataset into train and validation sets\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "print(f\"Train Data: {len(train_dataset)}\")\n",
        "print(f\"Validation Data: {len(val_dataset)}\")\n",
        "print(f\"Test Data: {len(test_dataset)}\")\n",
        "\n",
        "# Create DataLoaders for train, validation, and test sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Setup: Define the Pretrained ViT Model for Fine-tuning\n",
        "\n",
        "We'll use a pre-trained Vision Transformer and adapt it to our specific task (e.g., classifying 10 types of objects). Let's break down each line and its purpose:\n",
        "\n",
        "#### 1. Model Initialization\n",
        "```python\n",
        "model = vit_b_16(pretrained=True)\n",
        "```\n",
        "- **Purpose**: This line initializes the `vit_b_16` model, a Vision Transformer, with weights that have been pretrained on a large dataset (commonly ImageNet). Using a pretrained model as the starting point allows leveraging learned features which can considerably improve the model's performance on similar tasks with less data.\n",
        "- **Functionality**: The parameter `pretrained=True` instructs PyTorch to load the model complete with weights that have already been optimized. This pretraining helps in accelerating the training process and improving the model's accuracy, especially when the target dataset is relatively small or not diverse.\n",
        "\n",
        "#### 2. Adjusting the Classifier Head\n",
        "```python\n",
        "model.heads.head = nn.Linear(model.heads.head.in_features, epochs)  # Adjust for CIFAR-10\n",
        "```\n",
        "- **Purpose**: This line modifies the output layer of the `vit_b_16` model to suit the number of classes in the CIFAR-10 dataset. The CIFAR-10 dataset consists of 10 classes, thus the output layer must produce 10 outputs.\n",
        "- **Explanation**: In the original `vit_b_16` model, the classifier (also known as the \"head\") is designed to match the number of classes it was originally trained on (typically 1000 for ImageNet). This line changes the final linear layer so that it now has an output size equal to the number of epochs specified (10, assuming `epochs` variable is meant to represent the number of classes, which appears to be a coding error or a misuse of the variable name). It should typically be a direct numeric value representing the number of target classes, like `10` for CIFAR-10.\n",
        "- **Details**:\n",
        "    - `model.heads.head.in_features`: This retrieves the number of input features from the previous output layer, which remains unchanged and matches the model's internal architecture.\n",
        "    - `nn.Linear`: This creates a new linear layer with the number of input features equal to the original output layer's input features, and the number of output features set to the number of classes in the target dataset (intended to be 10, not `epochs`).\n",
        "\n",
        "The second line contains a likely mistake where `epochs` (a variable representing the number of training cycles) is used instead of the intended number of classes for CIFAR-10, which is 10. Correctly, this line should be written as:\n",
        "```python\n",
        "model.heads.head = nn.Linear(model.heads.head.in_features, 10)  # Correct number of classes for CIFAR-10\n",
        "```\n",
        "This adjustment ensures that the model's output can be directly used to classify images into the ten categories of the CIFAR-10 dataset.\n",
        "\n",
        "Reference: [vit_b_16](https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html) in PyTorch."
      ],
      "metadata": {
        "id": "15zBTq62FvSt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaXNJG48fCQD",
        "outputId": "ebe9977e-6211-4329-ba56-d89b6fdec23d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
            "100%|██████████| 330M/330M [00:01<00:00, 185MB/s]\n"
          ]
        }
      ],
      "source": [
        "model = vit_b_16(pretrained=True)\n",
        "model.heads.head = nn.Linear(model.heads.head.in_features, epochs)  # Adjust for CIFAR-10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "model.to(device)\n",
        "if device == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StNWDwOU2o3w",
        "outputId": "8121a6f0-753b-437e-e409-f0101645ad0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the loss function and the optimizer\n",
        "\n",
        "In the tutorial on fine-tuning a Vision Transformer for robotics applications using PyTorch, we will employ three crucial lines of code that establish the foundation for the model's training process. These lines define the loss function, the optimizer, and the learning rate scheduler. Here’s a breakdown of each component:\n",
        "\n",
        "#### 1. Loss Function\n",
        "```python\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "```\n",
        "- **Purpose**: This line of code defines the loss function that we will use to evaluate the difference between the model's predictions and the actual labels. `CrossEntropyLoss` is widely used for classification tasks because it combines `LogSoftmax` and `NLLLoss` in a single class.\n",
        "- **Functionality**: It calculates the loss between the model outputs and the target labels, which is essential for training as it provides a measure to optimize during the learning process.\n",
        "\n",
        "#### 2. Optimizer\n",
        "```python\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "```\n",
        "- **Purpose**: This line initializes the optimizer, which is responsible for updating the model parameters based on the computed gradients. We use Adam, an adaptive learning rate optimization algorithm, which is popular due to its efficiency in handling sparse gradients and its adaptiveness in different contexts.\n",
        "- **Functionality**: `Adam` stands for Adaptive Moment Estimation. It maintains a learning rate for each model parameter and adapts it throughout the training process. This helps in achieving faster convergence and reduces the chance of getting stuck in local optima.\n",
        "\n",
        "#### 3. Learning Rate Scheduler\n",
        "```python\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
        "```\n",
        "- **Purpose**: The learning rate scheduler adjusts the learning rate during training, which can lead to better performance and quicker convergence. `StepLR` decreases the learning rate by a factor of `gamma` every `step_size` epochs.\n",
        "- **Functionality**: This scheduler reduces the learning rate by multiplying it with a factor of `gamma` (a number less than 1) every `step_size` epochs. This is useful for taking larger steps in the initial phase of training when the weights are far from their optimal values and smaller steps as we approach convergence, preventing overshooting of the target minimum.\n",
        "\n",
        "By integrating these components into your training loop, the model is equipped to effectively learn from the data, adapt its weights during training, and improve its accuracy in object recognition tasks for robotics applications."
      ],
      "metadata": {
        "id": "iDhYbEnTJ2Ri"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlfuT_K1TrPI"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "\n",
        "The `train_model` function is a comprehensive training loop designed for optimizing a machine learning model over several epochs. It involves both training and validation phases to adjust the model's weights and monitor its performance over time. Let's walk through each component of this function:\n",
        "\n",
        "#### Function Definition\n",
        "```python\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
        "```\n",
        "- **Parameters**:\n",
        "  - `model`: the neural network model that will be trained.\n",
        "  - `train_loader` & `val_loader`: DataLoader objects that provide batches of training and validation data, respectively.\n",
        "  - `criterion`: the loss function used to evaluate how well the model fits the data.\n",
        "  - `optimizer`: the mechanism for updating model weights based on the computed gradients.\n",
        "  - `num_epochs`: the number of complete passes through the entire training dataset.\n",
        "\n",
        "#### Initialization\n",
        "```python\n",
        "best_loss = float('inf')\n",
        "```\n",
        "- **Purpose**: Initializes the `best_loss` variable to infinity, which is used to track the lowest validation loss observed during training. This helps in saving the best model state.\n",
        "\n",
        "#### Training Mode\n",
        "```python\n",
        "model.train()\n",
        "```\n",
        "- **Effect**: Sets the model to training mode, which is necessary for layers like dropout and batch normalization that have distinct behavior during training vs. testing.\n",
        "\n",
        "#### Training Loop\n",
        "```python\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    start_time = time.time()\n",
        "    ...\n",
        "```\n",
        "- **Loop Structure**: Iterates over the number of epochs specified. Within each epoch, the function records the start time (for performance monitoring), performs training, evaluates validation loss, and checks for improvements.\n",
        "\n",
        "#### Training Phase\n",
        "```python\n",
        "for images, labels in train_loader:\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "```\n",
        "- **Operations**:\n",
        "  - **Data Loading**: Batches of images and labels are loaded from the training DataLoader.\n",
        "  - **Device Assignment**: Data is transferred to the appropriate device (CPU/GPU).\n",
        "  - **Gradient Reset**: Clears old gradients from the previous batch.\n",
        "  - **Forward Pass**: Computes predictions by passing images through the model.\n",
        "  - **Loss Calculation**: Computes the loss using the specified criterion.\n",
        "  - **Backpropagation**: Calculates the gradients of the loss with respect to model parameters.\n",
        "  - **Weight Update**: Adjusts the weights based on the gradients using the optimizer.\n",
        "  - **Accumulate Loss**: Adds up the loss for later averaging.\n",
        "\n",
        "#### Validation Phase\n",
        "```python\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    ...\n",
        "```\n",
        "- **Evaluation Mode and Gradient Lock**: Switches to evaluation mode and disables gradient computation to save memory and computations during the validation pass.\n",
        "- **Validation Operations**: Similar to the training phase, but here the model does not update its weights; it only evaluates the performance on the validation set.\n",
        "\n",
        "#### Scheduler and Model Saving\n",
        "```python\n",
        "scheduler.step()\n",
        "...\n",
        "torch.save(model.state_dict(), model_file_name)\n",
        "```\n",
        "- **Learning Rate Adjustment**: Updates the learning rate according to the schedule.\n",
        "- **Model Checkpointing**: If the current validation loss is the lowest observed, saves the model's parameters. This model can later be reloaded for further training or evaluation.\n",
        "\n",
        "#### Performance Monitoring\n",
        "```python\n",
        "end_time = time.time()\n",
        "epoch_time = end_time - start_time\n",
        "print(...)\n",
        "```\n",
        "- **Timing**: Measures how long each epoch takes, providing insight into the computational cost of training.\n",
        "\n",
        "#### Model Reset\n",
        "```python\n",
        "model.train()\n",
        "```\n",
        "- **Reset to Training Mode**: Ensures that the model is ready for the next epoch of training.\n",
        "\n",
        "This training loop is robust and includes all the necessary elements to effectively train, validate, and monitor a deep learning model's performance, ensuring it is both effective and efficient."
      ],
      "metadata": {
        "id": "MMFvhs_SJdAm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set Training Parameters\n",
        "\n",
        "In the context of fine-tuning a Vision Transformer model for object recognition in robotics using PyTorch, the following training settings play a crucial role in defining how the model learns from the data:\n",
        "\n",
        "#### 1. Batch Size\n",
        "```python\n",
        "batch_size = 64\n",
        "```\n",
        "- **Definition**: The batch size determines the number of training examples utilized in one iteration of the model training. For this setting, a batch size of 64 means that the model processes 64 images in each step of the training loop.\n",
        "- **Impact**: A larger batch size can lead to faster training by taking advantage of parallel processing capabilities of modern GPUs, but it might also require more memory and can affect the model's ability to generalize. Conversely, a smaller batch size offers more updates per epoch, which can improve generalization but slow down the training process.\n",
        "\n",
        "#### 2. Number of Epochs\n",
        "```python\n",
        "epochs = 10\n",
        "```\n",
        "- **Definition**: An epoch is a full cycle through the entire training dataset. This setting specifies that the training process will iterate through the whole dataset a total of 10 times.\n",
        "- **Impact**: More epochs generally allow the model more opportunities to learn and adjust its weights from the data, potentially improving accuracy. However, too many epochs might lead to overfitting, especially if the dataset is not diverse enough or is relatively small.\n",
        "\n",
        "#### 3. Learning Rate\n",
        "```python\n",
        "lr = 3e-5\n",
        "```\n",
        "- **Definition**: The learning rate defines the step size at which the optimizer updates the weights of the model during training. A learning rate of \\(3 \\times 10^{-5}\\) is used here.\n",
        "- **Impact**: The learning rate is crucial for training dynamics; too high a rate might cause the model to converge too quickly to a suboptimal solution, and too low a rate might slow down the training process, potentially leading to a stall in learning.\n",
        "\n",
        "#### 4. Learning Rate Decay Factor (Gamma)\n",
        "```python\n",
        "gamma = 0.7\n",
        "```\n",
        "- **Definition**: This setting specifies the factor by which the learning rate is multiplied at regular intervals, as defined by the learning rate scheduler. Here, the learning rate is multiplied by 0.7 at specified intervals.\n",
        "- **Impact**: Adjusting the learning rate during training can help in fine-tuning the model more effectively by slowing down the updates as the model approaches optimal solutions. This can prevent overshooting the minimum loss and can lead to more stable convergence.\n",
        "\n",
        "These parameters collectively define the framework for the training process, balancing the rate of learning, the computational efficiency, and the model's capacity to generalize from the training data to real-world scenarios in robotics applications."
      ],
      "metadata": {
        "id": "Y9_MZnC-JOQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "lr = 3e-5\n",
        "gamma = 0.7"
      ],
      "metadata": {
        "id": "wbv4hpsI_4sG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the training loop"
      ],
      "metadata": {
        "id": "HQBeNU6bKyVy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r03_9QW9WBq1"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "best_loss = float('inf')\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
        "    global best_loss  # To allow updates to best_loss across epochs\n",
        "    model.train()  # Ensure the model is in training mode\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        start_time = time.time()  # Start timer\n",
        "        # Training phase\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        end_time = time.time()  # End timer\n",
        "        epoch_time = end_time - start_time  # Calculate epoch time\n",
        "\n",
        "        # Compute average training loss\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        # print(f'Epoch {epoch+1}, Training Loss: {avg_train_loss}')\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        val_running_loss = 0.0\n",
        "        with torch.no_grad():  # Disable gradient computation during validation\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_running_loss += loss.item()\n",
        "\n",
        "        # Compute average validation loss\n",
        "        avg_val_loss = val_running_loss / len(val_loader)\n",
        "        scheduler.step()  # Update the learning rate scheduler\n",
        "        print(f'Epoch {epoch+1}, Training Loss: {np.round(avg_train_loss, 6)}, Validation Loss: {np.round(avg_val_loss, 6)}, Training Time: {np.round(epoch_time, 2)}')\n",
        "\n",
        "        # Check if the current validation loss is the best we've seen so far\n",
        "        if avg_val_loss < best_loss:\n",
        "            best_loss = avg_val_loss\n",
        "            # model_file_name = f'best_model_{epoch+1}_2402422_v01.pt'\n",
        "            model_file_name = 'best_model_2402422_v01.pt'\n",
        "            torch.save(model.state_dict(), model_file_name)  # Save the best model\n",
        "            torch.save(model.state_dict(), '/content/drive/MyDrive/ViT_models/' + model_file_name)  # Save the best model\n",
        "            print('New best model saved!')\n",
        "\n",
        "        model.train()  # Set the model back to training mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyhut7J3TpBY",
        "outputId": "20bfa5d8-feb4-4358-9e6e-f43052b3cb13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 0.181454, Validation Loss: 0.088346, Training Time: 654.71\n",
            "New best model saved!\n",
            "Epoch 2, Training Loss: 0.033748, Validation Loss: 0.080057, Training Time: 655.47\n",
            "New best model saved!\n",
            "Epoch 3, Training Loss: 0.011561, Validation Loss: 0.067019, Training Time: 655.75\n",
            "New best model saved!\n",
            "Epoch 4, Training Loss: 0.004308, Validation Loss: 0.071983, Training Time: 655.46\n",
            "Epoch 5, Training Loss: 0.001347, Validation Loss: 0.068002, Training Time: 655.27\n",
            "Epoch 6, Training Loss: 0.000658, Validation Loss: 0.068222, Training Time: 655.29\n",
            "Epoch 7, Training Loss: 0.000524, Validation Loss: 0.0693, Training Time: 654.86\n",
            "Epoch 8, Training Loss: 0.000448, Validation Loss: 0.070445, Training Time: 655.09\n",
            "Epoch 9, Training Loss: 0.000398, Validation Loss: 0.071466, Training Time: 655.13\n",
            "Epoch 10, Training Loss: 0.00036, Validation Loss: 0.072017, Training Time: 655.13\n"
          ]
        }
      ],
      "source": [
        "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "To evaluate the model, check its performance on the test dataset. The function `evaluate_model` computes the model's accuracy, which is the percentage of correctly predicted instances relative to the total number of instances evaluated. Here's a breakdown of how this function works:\n",
        "\n",
        "### Function Definition\n",
        "```python\n",
        "def evaluate_model(model, test_loader):\n",
        "```\n",
        "- **Parameters**:\n",
        "  - `model`: the neural network model that will be evaluated.\n",
        "  - `test_loader`: a DataLoader object that provides batches of the test dataset, including both the input images and their corresponding labels.\n",
        "\n",
        "### Set Model to Evaluation Mode\n",
        "```python\n",
        "model.eval()\n",
        "```\n",
        "- **Purpose**: This line sets the model to evaluation mode, which is crucial for models that have different behavior during training and testing, such as those using dropout layers or batch normalization. In evaluation mode, these layers will behave consistently and not apply randomness or scaling.\n",
        "\n",
        "### Initialize Counters\n",
        "```python\n",
        "total = 0\n",
        "correct = 0\n",
        "```\n",
        "- **Usage**:\n",
        "  - `total`: keeps track of the total number of examples processed.\n",
        "  - `correct`: counts the number of examples for which the model's prediction matches the actual label.\n",
        "\n",
        "### Evaluation Loop\n",
        "```python\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "```\n",
        "- **Context**:\n",
        "  - **`torch.no_grad()`**: This context manager disables gradient computation, reducing memory usage and speeding up the process since gradients are not needed for model evaluation.\n",
        "  - **Data Movement**:\n",
        "    - `images.to(device), labels.to(device)`: Moves the data to the appropriate computing device (CPU or GPU), which is necessary for models trained on GPUs.\n",
        "  - **Model Prediction**:\n",
        "    - `outputs = model(images)`: Feeds the batch of images into the model and gets the output logits for each class.\n",
        "    - `_, predicted = torch.max(outputs.data, 1)`: Finds the predicted class label for each image by selecting the class with the highest logit value. The `torch.max` function returns both the maximum value and the index of that value (the predicted class label) across the specified dimension (`1`, meaning row-wise operation).\n",
        "  - **Update Counters**:\n",
        "    - `total += labels.size(0)`: Updates the total number of examples processed.\n",
        "    - `correct += (predicted == labels).sum().item()`: Increases the count of correct predictions by the number of images in the current batch where the prediction matched the label.\n",
        "\n",
        "### Calculate and Print Accuracy\n",
        "```python\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy on test images: {accuracy}%')\n",
        "```\n",
        "- **Calculation**:\n",
        "  - Computes the percentage of correct predictions relative to the total number of predictions made.\n",
        "- **Output**:\n",
        "  - Prints the computed accuracy to provide feedback on how well the model is performing on the unseen test data.\n",
        "\n",
        "This evaluation loop provides a straightforward and effective way to assess the accuracy of a model, allowing for the quantification of model performance in practical and operational terms."
      ],
      "metadata": {
        "id": "EsJYD3tQJ-bZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the evaluation loop on the best model"
      ],
      "metadata": {
        "id": "MUNkR8FTNyB7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2iQR8JN0fH7B"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy on test images: {accuracy}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6KF0iP7T2sq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3872ff84-910e-4752-a457-80c6aa1c792c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Load the best model\n",
        "model_file_name = 'best_model_2402422_v01.pt'\n",
        "best_model_state_dict = torch.load(model_file_name)\n",
        "model.load_state_dict(best_model_state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ww6Z2Z8gfJW-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8844c50c-02ae-40a1-c5d7-2d88380d7abe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test images: 97.65%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "evaluate_model(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results Analysis\n",
        "\n",
        "The reported accuracy of \"97.65%\" on test images reflects a very good performance for the Vision Transformer model, particularly when considering the complexities and variabilities associated with image recognition tasks. Achieving such a high level of accuracy signifies that the model has effectively learned from the training data and can generalize well to new, unseen images, which is crucial for practical applications.\n",
        "\n",
        "However, it is important to note that while this accuracy is commendable, it does not reach the state-of-the-art levels where accuracies exceed 99.5%. Such high-performance benchmarks are typically achieved by models that have undergone extensive fine-tuning on very large datasets and with substantial computational resources. These state-of-the-art models often involve:\n",
        "- More complex architectures or ensemble methods that integrate outputs from multiple models to boost accuracy.\n",
        "- Longer training times with numerous epochs, which allow the model to iteratively refine its weights and biases to better fit the data.\n",
        "- Advanced regularization techniques and hyperparameter optimization strategies that can significantly improve model performance but require experimental tuning and computational power.\n",
        "\n",
        "Reaching these top-tier accuracies usually demands considerable GPU compute power and time, making them less feasible within the constraints of a limited budget, as often is the case in educational or small-scale research settings. For the purposes of this tutorial, achieving an accuracy of 97.65% with the available resources and within a reasonable time frame is an impressive outcome. It demonstrates the capability of Vision Transformers to handle complex visual tasks effectively, offering a solid foundation for further exploration and optimization with more resources or in applications where very high accuracy is not the critical factor.\n",
        "\n",
        "In summary, while the model does not achieve the pinnacle of current machine learning performance, it provides a robust and highly effective solution for many practical applications, especially where budget and computational resources are constrained.\n",
        "\n",
        "State of the art (SOTA) benchmarks: [Image Classification on CIFAR-10\n",
        "](https://paperswithcode.com/sota/image-classification-on-cifar-10)"
      ],
      "metadata": {
        "id": "paggqzRyOTJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions\n",
        "\n",
        "In conclusion, this tutorial has demonstrated the process of fine-tuning a Vision Transformer (ViT) model, specifically the `vit_b_16`, for object recognition tasks relevant to robotics applications using PyTorch. Starting from setting up the necessary environment and data, through training and validation, to evaluating the model on a test dataset, each step has been carefully explained and implemented.\n",
        "\n",
        "Through our efforts, the model achieved an impressive accuracy of 97.65% on test images. This level of performance underscores the potential of Vision Transformers in image recognition tasks, showcasing their ability to competently handle visual data and make accurate predictions. While not reaching the state-of-the-art accuracy levels above 99.5%, which often require extensive computational resources and fine-tuning, the results achieved are significant, especially considering the practical constraints of limited GPU compute and budget.\n",
        "\n",
        "The tutorial also highlighted critical elements in training deep learning models, such as the importance of a well-considered loss function, optimizer, and learning rate scheduler. The discussions around the impact of batch size, number of epochs, learning rate, and data transformations have provided deeper insights into model training dynamics and the optimization process.\n",
        "\n",
        "Moving forward, participants can experiment with different architectures, tuning parameters, or more extensive datasets to push the boundaries of what can be achieved with their models. They could also explore the integration of these trained models into actual robotic systems, examining how these models perform in real-world scenarios and potentially iterating on design choices based on practical feedback.\n",
        "\n",
        "We hope this tutorial has provided you with a solid foundation in using Vision Transformers for image recognition and inspired you to delve deeper into the field of machine learning and robotics. Whether for academic, personal, or commercial purposes, the skills and knowledge gained here should serve as a robust base for further exploration and development in the exciting intersection of AI and robotics.\n"
      ],
      "metadata": {
        "id": "Z-oHVO3NKB0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Resources\n",
        "\n",
        "- [Hugging Face ViT](https://huggingface.co/docs/transformers/en/model_doc/vit)\n",
        "- [PyTorch ViT](https://pytorch.org/vision/main/models/vision_transformer.html)\n",
        "- [D2L AI - Attention Mechanisms and Transformers](https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html)\n",
        "- https://github.com/lucidrains/vit-pytorch"
      ],
      "metadata": {
        "id": "Sip7_yBuIq_o"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "17Y1yt-Rh1ukSAGcf0IXnbWmw17W30O9W",
      "authorship_tag": "ABX9TyODmQZoYvtUrOYNLq/fYs85",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}